from qcloud_cos import CosConfig, CosS3Client
import redis
from redis import RedisError
import asyncio
import json
import logging
from concurrent.futures import ThreadPoolExecutor
import functools
import backoff
from aiohttp import ClientError, ClientSession, ClientTimeout
from sqlalchemy import create_engine
from sqlalchemy import Column, String, Text, Integer, DateTime, Boolean, Float, ForeignKey, BigInteger
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
base = declarative_base()
engine = create_engine('mysql+pymysql://spiderman:ew4%2598fRpe@43.157.134.155:33070/spider')
Session = sessionmaker(bind=engine)

redis_client = redis.from_url('redis://:wnhmsy@43.157.146.240:6379/15')

# ä»»åŠ¡æ± é…ç½®
TASK_POOL_SIZE = 20
task_pool = asyncio.Queue(maxsize=TASK_POOL_SIZE) # 

# æ‰¹é‡ä¸‹è½½é…ç½®
WORKER_COUNT = 10  # å¹¶å‘å·¥ä½œå™¨æ•°é‡
BATCH_SIZE = 5     # æ¯æ‰¹å¤„ç†çš„ä»»åŠ¡æ•°é‡

# ç»Ÿè®¡ä¿¡æ¯
stats = {
    'processed': 0,
    'success': 0,
    'failed': 0,
    'start_time': None
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
    'Accept': '*/*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

client = CosS3Client(
    CosConfig(
        SecretId='IKIDNeZR7cw2zmQr68xazIPK3wgEZgZhfT5g',
        SecretKey='2rUvgHmAPaaMNEsOksnYAh3B7vWt2ylM',
        Region='sa-saopaulo'
    )
)
upload_concurrency = 20
semaphore = asyncio.Semaphore(upload_concurrency)
executor = ThreadPoolExecutor()

class CXTV_img(base):

    __tablename__ = 'cxtv_img'

    img_id = Column(Integer, primary_key=True, autoincrement=True)
    cxtv_id = Column(Integer, nullable=False)
    url = Column(String(255), nullable=False)
    storage = Column(String(255), nullable=False)
    status = Column(Integer, nullable=False)


def log_retry_error(details):
    logger = logging.getLogger('AsyncCosUploader')
    logger.error(f"Retrying {details['tries']}: \nexception={details['exception']} \nargs={details['args']}")

aio_retry = functools.partial(
    backoff.on_exception,
    backoff.expo,
    (Exception, ),
    max_tries=3,
    on_backoff=log_retry_error,
    logger=logging.getLogger('AsyncCosUploader')
)()

@aio_retry
async def upload_object(content, cos_path):
    async with semaphore:
        await run_with_executor(put_object, content, cos_path)


def put_object(content, key):
    
    client.put_object(
        Bucket='media-1347269025',
        Body=content,
        Key=key
    )

        
    
        
async def run_with_executor(func, *args):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, func, *args)


async def get_next_task():
    """å¼‚æ­¥è·å–Redisä»»åŠ¡"""
    try:
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(executor, redis_client.brpop, 'cxtv_img', 1)
        if result:
            _, task = result
            return json.loads(task)
    except RedisError as e:
        logging.error(f"Redis error: {e}")
        return None
    except Exception as e:
        logging.error(f"è·å–ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        return None


async def fill_task_pool():
    """å¡«å……ä»»åŠ¡æ± ï¼Œå½“æ± å­æ»¡äº†å°±ç­‰å¾…"""
    while True:
        try:
            # å¦‚æœæ± å­æ»¡äº†ï¼Œç­‰å¾…æœ‰ç©ºé—´
            if task_pool.full():
                logging.info("ä»»åŠ¡æ± å·²æ»¡ï¼Œç­‰å¾…å¤„ç†...")
                await asyncio.sleep(1)
                continue
            
            # ä»Redisè·å–ä»»åŠ¡
            task = await get_next_task()
            if task:
                # å°†ä»»åŠ¡æ”¾å…¥æ± å­ï¼Œå¦‚æœæ± å­æ»¡äº†ä¼šé˜»å¡
                await task_pool.put(task)
                logging.info(f"ä»»åŠ¡å·²æ·»åŠ åˆ°æ± å­ï¼Œå½“å‰æ± å­å¤§å°: {task_pool.qsize()}")
            else:
                # æ²¡æœ‰ä»»åŠ¡æ—¶çŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.1)
                
        except Exception as e:
            logging.error(f"å¡«å……ä»»åŠ¡æ± æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(1)


async def get_task_from_pool():
    """ä»ä»»åŠ¡æ± è·å–ä»»åŠ¡ï¼Œå¦‚æœæ± å­ç©ºäº†å°±ç­‰å¾…"""
    try:
        # ä»æ± å­è·å–ä»»åŠ¡ï¼Œå¦‚æœæ± å­ç©ºäº†ä¼šé˜»å¡ç­‰å¾…
        task = await task_pool.get()
        logging.info(f"ä»æ± å­è·å–ä»»åŠ¡ï¼Œå‰©ä½™ä»»åŠ¡æ•°: {task_pool.qsize()}")
        return task
    except Exception as e:
        logging.error(f"ä»ä»»åŠ¡æ± è·å–ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        return None


async def main():
    """ä¸»å‡½æ•°ï¼Œå¯åŠ¨ä»»åŠ¡æ± ç®¡ç†"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
    stats['start_time'] = asyncio.get_event_loop().time()
    logging.info(f"ğŸš€ å¯åŠ¨æ‰¹é‡ä¸‹è½½ç³»ç»Ÿ: {WORKER_COUNT}ä¸ªå·¥ä½œå™¨, æ‰¹é‡å¤§å°={BATCH_SIZE}")
    
    # åˆ›å»ºä»»åŠ¡å¡«å……å™¨
    fill_task = asyncio.create_task(fill_task_pool())
    
    # åˆ›å»ºå¤šä¸ªæ‰¹é‡ä»»åŠ¡å¤„ç†å™¨
    batch_workers = []
    for i in range(WORKER_COUNT):
        worker_task = asyncio.create_task(process_batch_tasks(f"batch-worker-{i}"))
        batch_workers.append(worker_task)
    
    # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯æ‰“å°å™¨
    stats_task = asyncio.create_task(print_stats())
    
    try:
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(fill_task, *batch_workers, stats_task)
    except KeyboardInterrupt:
        logging.info("ğŸ›‘ æ­£åœ¨åœæ­¢æ‰¹é‡ä¸‹è½½ç³»ç»Ÿ...")
        fill_task.cancel()
        for task in batch_workers:
            task.cancel()
        stats_task.cancel()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        if stats['start_time']:
            elapsed = (asyncio.get_event_loop().time() - stats['start_time'])
            rate = stats['processed'] / elapsed if elapsed > 0 else 0
            logging.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ€»å¤„ç†={stats['processed']}, æˆåŠŸ={stats['success']}, å¤±è´¥={stats['failed']}, å¹³å‡é€Ÿç‡={rate:.2f} ä»»åŠ¡/ç§’")
    except Exception as e:
        logging.error(f"ä¸»ç¨‹åºå‡ºé”™: {e}")


async def process_tasks(worker_name="default"):
    """å¤„ç†ä»»åŠ¡çš„ä¸»å¾ªç¯ - æ”¯æŒå¤šä¸ªå·¥ä½œå™¨å¹¶å‘å¤„ç†"""
    while True:
        try:
            # ä»ä»»åŠ¡æ± è·å–ä»»åŠ¡
            task = await get_task_from_pool()
            if task:
                # å¤„ç†ä»»åŠ¡
                await process_single_task(task, worker_name)
            else:
                await asyncio.sleep(0.1)
        except Exception as e:
            logging.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™ [{worker_name}]: {e}")
            await asyncio.sleep(1)


async def process_single_task(task, worker_name="default"):
    """å¤„ç†å•ä¸ªä»»åŠ¡"""
    try:
        # è¿™é‡Œæ·»åŠ ä½ çš„ä»»åŠ¡å¤„ç†é€»è¾‘
        logging.info(f"[{worker_name}] å¤„ç†ä»»åŠ¡: {task}")
        url = task['url']
        storage = task['storage']

        await download_and_upload_image(url=url, cos_path=storage, worker_name=worker_name)
        await update_success(task['img_id'])

    except Exception as e:
        logging.error(f"[{worker_name}] å¤„ç†ä»»åŠ¡ {task} æ—¶å‡ºé”™: {e}")


async def update_success(id):
    # try:
    #     with Session() as db:
    #         db.query(CXTV_img).filter_by(img_id=id).update({'status': 2}) # ç”¨äºæ§åˆ¶åœ¨æ‰§è¡Œæ‰¹é‡æ“ä½œï¼ˆå¦‚æ‰¹é‡æ›´æ–°æˆ–åˆ é™¤ï¼‰æ—¶å¦‚ä½•å¤„ç†å½“å‰ä¼šè¯ä¸­çš„å¯¹è±¡çŠ¶æ€
    #     db.commit()
    #     logging.info(f"æ›´æ–°æˆåŠŸ:img_id = {id}")
    # except Exception as e:
    #     logging.error(f"æ›´æ–°æˆåŠŸæ—¶å‡ºé”™: {e}")
    try:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(executor, _update_success_sync, id)
        logging.info(f"æ›´æ–°æˆåŠŸ: img_id={id}")
    except Exception as e:
        logging.error(f"æ›´æ–°æˆåŠŸæ—¶å‡ºé”™: {e}")


def _update_success_sync(id):
    """åŒæ­¥æ›´æ–°æ•°æ®åº“çŠ¶æ€"""
    try:
        with Session() as db:
            db.query(CXTV_img).filter_by(img_id=id).update({'status': 2})
            db.commit()
            logging.info(f"æ•°æ®åº“æ›´æ–°æˆåŠŸ: img_id={id}")
    except Exception as e:
        logging.error(f"æ•°æ®åº“æ›´æ–°å¤±è´¥: img_id={id}, error={e}")
        raise

async def download_and_upload_image(url: str, cos_path: str, worker_name="default"):
        logging.info(f"[{worker_name}] [æ­£åœ¨ä¸‹è½½] {cos_path}")
        content = await fetch(url)
        logging.info(f"[{worker_name}] [ä¸‹è½½æˆåŠŸ] {cos_path} {len(content)} bytes")
        await upload_object(content, cos_path)
        logging.info(f"[{worker_name}] [ä¸Šä¼ æˆåŠŸ] {cos_path}")

async def fetch(url):
    timeout = ClientTimeout(total=30)
    async with ClientSession(
        raise_for_status=True,
        headers=headers,
        timeout=timeout
    ) as session:
        async with session.get(
            url=url,
            headers=headers
        ) as resp:
            return await resp.read()


async def process_batch_tasks(worker_name="default"):
    """æ‰¹é‡å¤„ç†ä»»åŠ¡ - ä¸€æ¬¡å¤„ç†å¤šä¸ªä»»åŠ¡"""
    while True:
        try:
            batch_tasks = []
            
            # æ”¶é›†ä¸€æ‰¹ä»»åŠ¡
            for _ in range(BATCH_SIZE):
                try:
                    task = await asyncio.wait_for(get_task_from_pool(), timeout=1.0)
                    if task:
                        batch_tasks.append(task)
                except asyncio.TimeoutError:
                    break
            
            if not batch_tasks:
                await asyncio.sleep(0.1)
                continue
            
            logging.info(f"[{worker_name}] å¼€å§‹æ‰¹é‡å¤„ç† {len(batch_tasks)} ä¸ªä»»åŠ¡")
            
            # å¹¶å‘å¤„ç†è¿™æ‰¹ä»»åŠ¡
            tasks = [process_single_task(task, worker_name) for task in batch_tasks]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    stats['failed'] += 1
                else:
                    stats['success'] += 1
                stats['processed'] += 1
            
            logging.info(f"[{worker_name}] æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ={len([r for r in results if not isinstance(r, Exception)])}, å¤±è´¥={len([r for r in results if isinstance(r, Exception)])}")
            
        except Exception as e:
            logging.error(f"[{worker_name}] æ‰¹é‡å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(1)


async def print_stats():
    """å®šæœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    while True:
        try:
            if stats['start_time']:
                elapsed = (asyncio.get_event_loop().time() - stats['start_time'])
                rate = stats['processed'] / elapsed if elapsed > 0 else 0
                logging.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å·²å¤„ç†={stats['processed']}, æˆåŠŸ={stats['success']}, å¤±è´¥={stats['failed']}, é€Ÿç‡={rate:.2f} ä»»åŠ¡/ç§’")
            await asyncio.sleep(10)  # æ¯10ç§’æ‰“å°ä¸€æ¬¡
        except Exception as e:
            logging.error(f"æ‰“å°ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            await asyncio.sleep(10)


def configure_batch_download(worker_count=10, batch_size=5, task_pool_size=20):
    """é…ç½®æ‰¹é‡ä¸‹è½½å‚æ•°"""
    global WORKER_COUNT, BATCH_SIZE, TASK_POOL_SIZE, task_pool
    
    WORKER_COUNT = worker_count
    BATCH_SIZE = batch_size
    TASK_POOL_SIZE = task_pool_size
    
    # é‡æ–°åˆ›å»ºä»»åŠ¡æ± 
    task_pool = asyncio.Queue(maxsize=TASK_POOL_SIZE)
    
    logging.info(f"âš™ï¸ æ‰¹é‡ä¸‹è½½é…ç½®å·²æ›´æ–°: å·¥ä½œå™¨={WORKER_COUNT}, æ‰¹é‡å¤§å°={BATCH_SIZE}, æ± å­å¤§å°={TASK_POOL_SIZE}")


if __name__ == "__main__":
    # é…ç½®æ‰¹é‡ä¸‹è½½å‚æ•°ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    configure_batch_download(
        worker_count=10,    # 10ä¸ªå¹¶å‘å·¥ä½œå™¨
        batch_size=5,       # æ¯æ‰¹å¤„ç†5ä¸ªä»»åŠ¡
        task_pool_size=20   # ä»»åŠ¡æ± å¤§å°20
    )
    
    asyncio.run(main())

