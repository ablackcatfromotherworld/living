#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Livescore Scrapy çˆ¬è™«è¿è¡Œè„šæœ¬
"""

import os
import sys
import argparse
import logging
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
project_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_dir)

from spiders.livescore_spider import LivescoreSpider


def setup_logging(log_level='INFO'):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler('livescore_scrapy.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Livescore Scrapy æ•°æ®çˆ¬è™«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ä½¿ç”¨ç¤ºä¾‹:
  python run_spider.py                                    # è¿è¡Œé»˜è®¤çˆ¬è™«
  python run_spider.py --sports soccer,basketball         # æŒ‡å®šè¿åŠ¨ç±»å‹
  python run_spider.py --languages pt,es,en               # æŒ‡å®šè¯­è¨€
        """
    )
    
    parser.add_argument(
        '--sports',
        help='è¿åŠ¨ç±»å‹ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: soccer,basketball,tennis'
    )
    
    parser.add_argument(
        '--languages',
        help='è¯­è¨€ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: pt,es,en'
    )
    
    parser.add_argument(
        '--days-back',
        type=int,
        default=10,
        help='å‘å‰è·å–å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤10å¤©ï¼‰'
    )
    
    parser.add_argument(
        '--days-forward',
        type=int,
        default=30,
        help='å‘åè·å–å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤30å¤©ï¼‰'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=100,
        help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 100)'
    )
    
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )
    
    parser.add_argument(
        '--database-url',
        default='mysql+pymysql://spider_0818:3IQ6fgAQVad0PylSEg.@43.157.134.155:33070/spider',
        help='æ•°æ®åº“è¿æ¥URL'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    # è·å–Scrapyè®¾ç½®
    settings = get_project_settings()
    
    # æ›´æ–°è®¾ç½®
    settings.set('DATABASE_URL', args.database_url)
    settings.set('BATCH_SIZE', args.batch_size)
    settings.set('LOG_LEVEL', args.log_level)
    
    # ä½¿ç”¨é‡æ„åçš„ç®¡é“
    settings.set('ITEM_PIPELINES', {
        'pipelines.refactored_database_pipeline.RefactoredDatabasePipeline': 400,
    })
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # æ·»åŠ çˆ¬è™«
    process.crawl(
        LivescoreSpider,
        sports=args.sports,
        languages=args.languages,
        days_back=args.days_back,
        days_forward=args.days_forward
    )
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™«...")
    logger.info(f"ğŸŒ æ•°æ®åº“: {args.database_url.split('@')[1] if '@' in args.database_url else 'localhost'}")
    
    # è¿è¡Œçˆ¬è™«
    try:
        process.start()
        logger.info("âœ… çˆ¬è™«è¿è¡Œå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    sys.exit(main())